# neural-networks
COMP 237 - Assignment 5 - Fall 2023

                                        
### Comparison of Single layered and multilayered networks (Exercise #1 and Exercise #2):

In comparing Result #1[0.20190123], generated from a single-layer feedforward network, and Result #2 [-0.64014285], derived from a more complex two-layer feedforward network, several key observations emerge. Result #1 employs a simpler architecture with six neurons in the hidden layer, achieving successful training with a minimized error of 0.00001 for the provided test values. On the other hand, Result #2 introduces increased complexity with two hidden layers and performs well in minimizing the error through Gradient Descent Backpropagation. However, the significant change in prediction outcome, from a positive sum in Result #1 to a negative sum in Result #2, underscores the network's enhanced capability to capture intricate relationships within the data. This change in sign suggests that the additional layers and neurons in Result #2 introduce greater flexibility but may also lead to different predictions. The choice between these models depends on the specific requirements of the task, with Result #1 being suitable for simpler tasks where interpretability is crucial, and Result #2 potentially beneficial for addressing more complex patterns, albeit with careful consideration to avoid overfitting

### Comparison of Single-layered with 10 random instances and Single-layered with 100 random instances (Exercise #1 and Exercise #3):

In Result #3, the method produces a larger dataset with 100 random occurrences for both input and output, while using the same range and summing logic as in Result #1. The neural network design is still a single-layer feedforward network with six neurons in the hidden layer and a single output. Result #3 shows a higher positive value of about [[0.29919379]] for the test values [0.1, 0.2] after 1000 epochs of training to minimize the error to 0.00001. When comparing Result #3 to Result #1, which yielded [[0.20190123]], the larger dataset appears to improve the network's capacity to generalize and make more accurate predictions. This observation highlights the significance of dataset size in training neural networks, suggesting that larger and more diverse datasets can contribute to improved model performance and a more refined understanding of underlying patterns.

### Comparison of Single-layered with 100 random instances and Multi-layered with 100 random instances (Exercise #4 and Exercise #3)

In Result #4, the provided code extends the analysis by employing a two-layer feedforward neural network with five neurons in the first hidden layer, three neurons in the second, and one output. Using the same dataset of 100 random examples for input and output as in Result #3, the network is trained for 1000 epochs using Gradient Descent Backpropagation. The training process is depicted by a plot of the training error, which shows the network's convergence over epochs. For the input values [0.1, 0.2], the test result for Result #4 produces a greater positive value of roughly [0.49986474]. When compared to Result #3, which yielded [0.29919379], the increase in the number of neurons in the hidden layers appears to improve the network's ability to catch more detailed patterns within the data, resulting in a distinct and higher prediction output. This observation highlights the crucial role of network architecture in shaping the model's predictive capabilities, emphasizing the trade-off between complexity and accuracy in selecting an appropriate neural network configuration for a given task. Further experimentation and analysis could provide additional insights into the behavior and performance of different architectures.

### Comparison of Result #5 and #6 for multi-layered feed (Exercise #5)

The comparison between Result #5 and Result #6, obtained from a three-input multi-layer feedforward neural network, reveals notable differences in the predicted sum patterns. Result #5 produces a prediction of approximately 0.27, while Result #6 significantly increases to around 0.72. The transition from two to three inputs in Result #6 introduces additional complexity to the model, allowing it to capture more intricate relationships among the input values. The higher predicted sum suggests that the neural network, with the expanded input dimensionality, is more sensitive to the nuances of the input patterns. This sensitivity underscores the importance of carefully selecting the number of input dimensions in neural network design, emphasizing the need for an appropriate balance between model complexity and the characteristics of the input data. The findings highlight the influence of input dimensionality on the network's predictive capabilities and stress the significance of understanding the underlying data structure for effective model design.

### Conclusion

- Exercise #1: Single-layer network successfully identifies the sum pattern with a simple architecture.
- Exercise #2: Introduction of a more complex two-layer network demonstrates varying prediction outcomes with increased model complexity.
- Exercise #3: Enlarging the dataset improves the performance of the single-layer network, emphasizing the importance of sufficient training data.
- Exercise #4: Expansion of the multi-layer network to handle a larger dataset highlights the trade-off between generalization and model complexity.
- Exercise #5: Introduction of a three-input multi-layer network demonstrates sensitivity to changes in input dimensionality. The contrast between Results #5 and #6 emphasizes how additional input features affect the predicted sum, highlighting the importance of considering input characteristics during model building.
Collectively, the exercises highlight the importance of dataset size, network architecture, and input dimensionality in influencing neural network performance. The selection of these parameters is crucial and should correspond to individual work needs. Iterative neural network development with constant testing and analysis is critical for improving models and obtaining optimal performance. Overall, this comprehensive study advances our knowledge of the dynamics involved in training and evaluating feedforward neural networks for pattern recognition tasks.
